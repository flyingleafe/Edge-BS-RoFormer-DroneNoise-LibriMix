{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data exploration: DN-LM and training pipeline\n",
        "\n",
        "This notebook lets you **listen to and inspect** the data fed to the models:\n",
        "1. **Raw DN-LM samples** — vocals, noise, and mixture as stored on disk (no chunking, no augmentations).\n",
        "2. **After the dataset pipeline** — the same data after chunking, optional loudness, and mixing (what the model sees as input and target).\n",
        "\n",
        "Run from the project root so `dataset` and `utils` import correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup: paths and config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample rate: 16000 Hz\n",
            "Chunk size: 131584 samples (8.22 s)\n",
            "Instruments: ['vocals', 'noise']\n",
            "Target: vocals\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "# Project root (adjust if running from elsewhere)\n",
        "PROJECT_ROOT = Path(os.getcwd())\n",
        "if (PROJECT_ROOT / \"dataset.py\").exists():\n",
        "    os.chdir(PROJECT_ROOT)\n",
        "\n",
        "# Paths — point to your DN-LM split and config\n",
        "DATA_PATH = PROJECT_ROOT / \"datasets\" / \"DN-LM\" / \"train\"\n",
        "VALID_PATH = PROJECT_ROOT / \"datasets\" / \"DN-LM\" / \"valid\"\n",
        "CONFIG_PATH = PROJECT_ROOT / \"configs\" / \"3_FA_RoPE(64).yaml\"\n",
        "\n",
        "# Where to cache dataset metadata (no training results needed)\n",
        "EXPLORE_CACHE = PROJECT_ROOT / \"results\" / \"explore\"\n",
        "EXPLORE_CACHE.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "from utils import load_config\n",
        "\n",
        "config = load_config(\"edge_bs_rof\", str(CONFIG_PATH))\n",
        "sample_rate = getattr(config.audio, \"sample_rate\", None) or getattr(config.audio, \"samplerate\", 16000)\n",
        "chunk_size = config.audio.chunk_size\n",
        "instruments = list(config.training.instruments)\n",
        "target_instrument = getattr(config.training, \"target_instrument\", None) or \"vocals\"\n",
        "\n",
        "print(f\"Sample rate: {sample_rate} Hz\")\n",
        "print(f\"Chunk size: {chunk_size} samples ({chunk_size / sample_rate:.2f} s)\")\n",
        "print(f\"Instruments: {instruments}\")\n",
        "print(f\"Target: {target_instrument}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Raw DN-LM samples (from disk)\n",
        "\n",
        "Load full-length **vocals**, **noise**, and **mixture** from a few sample folders. No chunking, no augmentations — exactly as created by `create_dataset.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/home/flyingleafe/Research/PhD/projects/Edge-BS-RoFormer-DroneNoise-LibriMix/datasets/DN-LM/valid'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# List sample folders (train or valid)\u001b[39;00m\n\u001b[32m     36\u001b[39m split_dir = DATA_PATH \u001b[38;5;28;01mif\u001b[39;00m DATA_PATH.exists() \u001b[38;5;28;01melse\u001b[39;00m VALID_PATH\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m sample_dirs = \u001b[38;5;28msorted\u001b[39m(\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msplit_dir\u001b[49m\u001b[43m.\u001b[49m\u001b[43miterdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstartswith\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sample_dirs:\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo sample folders in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Create DN-LM with create_dataset.py first.\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/iizfzmqjdfvbyx82gqw92glb5zl6m4my-python3-3.12.12/lib/python3.12/pathlib.py:1056\u001b[39m, in \u001b[36mPath.iterdir\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1050\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34miterdir\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1051\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Yield path objects of the directory contents.\u001b[39;00m\n\u001b[32m   1052\u001b[39m \n\u001b[32m   1053\u001b[39m \u001b[33;03m    The children are yielded in arbitrary order, and the\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[33;03m    special entries '.' and '..' are not included.\u001b[39;00m\n\u001b[32m   1055\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1056\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[32m   1057\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_child_relpath(name)\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/home/flyingleafe/Research/PhD/projects/Edge-BS-RoFormer-DroneNoise-LibriMix/datasets/DN-LM/valid'"
          ]
        }
      ],
      "source": [
        "def load_raw_sample(sample_dir, sr=None):\n",
        "    \"\"\"Load vocals, noise, mixture from one DN-LM sample folder. Returns (vocals, noise, mixture), sr.\"\"\"\n",
        "    sample_dir = Path(sample_dir)\n",
        "    vocals, sr_v = sf.read(sample_dir / \"vocals.wav\", dtype=\"float32\")\n",
        "    noise, sr_n = sf.read(sample_dir / \"noise.wav\", dtype=\"float32\")\n",
        "    mixture, sr_m = sf.read(sample_dir / \"mixture.wav\", dtype=\"float32\")\n",
        "    sr_actual = sr_v  # all same in DN-LM\n",
        "    if sr is not None and sr_actual != sr:\n",
        "        import librosa\n",
        "        vocals = librosa.resample(vocals, orig_sr=sr_actual, target_sr=sr)\n",
        "        noise = librosa.resample(noise, orig_sr=sr_actual, target_sr=sr)\n",
        "        mixture = librosa.resample(mixture, orig_sr=sr_actual, target_sr=sr)\n",
        "        sr_actual = sr\n",
        "    return vocals, noise, mixture, sr_actual\n",
        "\n",
        "\n",
        "def plot_waveforms(vocals, noise, mixture, sr, title=\"\", axs=None):\n",
        "    t = np.arange(len(mixture)) / sr\n",
        "    if axs is None:\n",
        "        fig, axs = plt.subplots(3, 1, figsize=(10, 6), sharex=True)\n",
        "    for ax, sig, label in zip(axs, [vocals, noise, mixture], [\"Vocals (target)\", \"Noise\", \"Mixture (input)\"]):\n",
        "        sig_1d = np.squeeze(sig)\n",
        "        if sig_1d.ndim > 1:\n",
        "            sig_1d = sig_1d.mean(axis=1)\n",
        "        ax.plot(t, sig_1d, color=\"#2e86ab\" if \"Vocals\" in label else \"#a23b72\" if \"Noise\" in label else \"#f18f01\")\n",
        "        ax.set_ylabel(label)\n",
        "        ax.set_xlim(0, t[-1])\n",
        "    axs[-1].set_xlabel(\"Time (s)\")\n",
        "    if title:\n",
        "        axs[0].set_title(title)\n",
        "    plt.tight_layout()\n",
        "    return axs\n",
        "\n",
        "\n",
        "# List sample folders (train or valid)\n",
        "split_dir = DATA_PATH if DATA_PATH.exists() else VALID_PATH\n",
        "sample_dirs = sorted([d for d in split_dir.iterdir() if d.is_dir() and not d.name.startswith(\".\")])\n",
        "if not sample_dirs:\n",
        "    raise FileNotFoundError(f\"No sample folders in {split_dir}. Create DN-LM with create_dataset.py first.\")\n",
        "\n",
        "# How many raw samples to show\n",
        "num_raw_samples = 3\n",
        "indices = np.linspace(0, len(sample_dirs) - 1, num_raw_samples, dtype=int)\n",
        "\n",
        "for idx in indices:\n",
        "    sample_dir = sample_dirs[idx]\n",
        "    vocals, noise, mixture, sr = load_raw_sample(sample_dir, sr=sample_rate)\n",
        "    fig, axs = plt.subplots(3, 1, figsize=(10, 6), sharex=True)\n",
        "    plot_waveforms(vocals, noise, mixture, sr, title=f\"Raw DN-LM: {sample_dir.name}\", axs=axs)\n",
        "    plt.show()\n",
        "    print(f\"\\n{sample_dir.name} — listen:\")\n",
        "    display(Audio(mixture, rate=sr))\n",
        "    display(Audio(np.squeeze(vocals), rate=sr))\n",
        "    display(Audio(np.squeeze(noise), rate=sr))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Metadata (SNR, sources)\n",
        "\n",
        "If DN-LM was created with `create_dataset.py`, each split has a `metadata.json` with SNR and source file names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metadata_path = split_dir / \"metadata.json\"\n",
        "if metadata_path.exists():\n",
        "    with open(metadata_path) as f:\n",
        "        meta = json.load(f)\n",
        "    # Key is 'train' or 'valid'\n",
        "    key = \"train\" if \"train\" in meta else \"valid\"\n",
        "    entries = meta[key]\n",
        "    print(f\"Metadata entries: {len(entries)}\")\n",
        "    for idx in indices[:3]:\n",
        "        e = entries[idx]\n",
        "        print(f\"  {e.get('id', idx)}: SNR={e.get('input_snr', 'N/A'):.2f} dB, speech_source={e.get('speech_source', '')}\")\n",
        "else:\n",
        "    print(f\"No metadata at {metadata_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. After the dataset pipeline (what the model sees)\n",
        "\n",
        "The training dataloader uses `MSSDataset`: it loads **chunks** of audio (random offset), optionally applies loudness per track, then sums sources into the mixture. With the default Edge-BS-RoFormer config, **augmentations are disabled**, so you see chunking + loudness only. Output is **(target, mix)** — target is the vocals track, mix is the model input.\n",
        "\n",
        "Shapes: `(2, chunk_size)` (stereo). For playback we use mono (mean over channels)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataset import MSSDataset\n",
        "\n",
        "data_path = str(DATA_PATH) if DATA_PATH.exists() else str(VALID_PATH)\n",
        "metadata_path = str(EXPLORE_CACHE / f\"metadata_1.pkl\")\n",
        "\n",
        "ds = MSSDataset(\n",
        "    config,\n",
        "    data_path,\n",
        "    metadata_path=metadata_path,\n",
        "    dataset_type=1,\n",
        "    batch_size=config.training.batch_size,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "# Get a few samples (target, mix) — each ds[i] is an independent random draw\n",
        "num_samples_show = 2\n",
        "\n",
        "for b in range(num_samples_show):\n",
        "    target, mix = ds[b]  # single sample: (2, chunk_size), (2, chunk_size)\n",
        "    target_np = target.numpy()  # (2, T)\n",
        "    mix_np = mix.numpy()\n",
        "    t = np.arange(chunk_size) / sample_rate\n",
        "    fig, axs = plt.subplots(2, 1, figsize=(10, 4), sharex=True)\n",
        "    for ax, sig, label in zip(axs, [target_np, mix_np], [\"Target (vocals)\", \"Mixture (input)\"]):\n",
        "        mono = np.squeeze(sig).mean(axis=0) if np.squeeze(sig).ndim > 1 else np.squeeze(sig)\n",
        "        ax.plot(t, mono, color=\"#2e86ab\" if \"Target\" in label else \"#f18f01\")\n",
        "        ax.set_ylabel(label)\n",
        "    axs[-1].set_xlabel(\"Time (s)\")\n",
        "    axs[0].set_title(f\"Dataset output (after chunking/loudness) — sample index {b}\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(f\"Listen — target then mixture:\")\n",
        "    display(Audio(target_np.mean(axis=0), rate=sample_rate))\n",
        "    display(Audio(mix_np.mean(axis=0), rate=sample_rate))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Side-by-side: one raw sample vs one dataset chunk\n",
        "\n",
        "Compare a **full raw mixture** from disk with a **chunk** from the dataset (different random draw). Duration of the chunk is `chunk_size / sample_rate` seconds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocals, noise, mixture, sr = load_raw_sample(sample_dirs[0], sr=sample_rate)\n",
        "target_chunk, mix_chunk = ds[0]\n",
        "target_chunk = target_chunk.numpy().mean(axis=0)\n",
        "mix_chunk = mix_chunk.numpy().mean(axis=0)\n",
        "\n",
        "fig, axs = plt.subplots(2, 1, figsize=(10, 5), sharex=False)\n",
        "t_raw = np.arange(len(mixture)) / sr\n",
        "t_chunk = np.arange(len(mix_chunk)) / sample_rate\n",
        "axs[0].plot(t_raw, np.squeeze(mixture).mean(axis=0) if np.squeeze(mixture).ndim > 1 else np.squeeze(mixture), color=\"#f18f01\", label=\"Raw mixture (full length)\")\n",
        "axs[0].set_ylabel(\"Amplitude\")\n",
        "axs[0].set_title(\"Raw mixture from disk\")\n",
        "axs[0].legend()\n",
        "axs[1].plot(t_chunk, mix_chunk, color=\"#f18f01\", label=\"Dataset mixture (chunk)\")\n",
        "axs[1].set_xlabel(\"Time (s)\")\n",
        "axs[1].set_ylabel(\"Amplitude\")\n",
        "axs[1].set_title(f\"Dataset mixture chunk ({len(mix_chunk)} samples = {len(mix_chunk)/sample_rate:.2f} s)\")\n",
        "axs[1].legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Raw mixture (full):\")\n",
        "display(Audio(np.squeeze(mixture), rate=sr))\n",
        "print(\"Dataset mixture (chunk):\")\n",
        "display(Audio(mix_chunk, rate=sample_rate))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
