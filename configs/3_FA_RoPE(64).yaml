audio:
  chunk_size: 131584      # Audio chunk size
  dim_f: 1024            # Frequency dimension size
  dim_t: 515             # Time dimension size
  hop_length: 512        # STFT hop length
  n_fft: 2048           # FFT size
  num_channels: 1        # Number of audio channels, 1 for mono
  sample_rate: 16000     # Sample rate
  min_mean_abs: 0.000    # Minimum mean absolute value

model:
  dim: 48               # Model hidden layer dimension, reduced from 384 to 48 to reduce parameters
  depth: 3              # Total Transformer layers, reduced from 12 to 3
  dim_head: 64          # Attention head dimension
  heads: 8              # Number of attention heads
  flash_attn: true      # Whether to use Flash Attention optimization
  use_rotary_pos: true  # Whether to use Rotary Position Encoding (RoPE)
  max_seq_len: 1000         # Maximum sequence length
  freqs_per_bands: !!python/tuple  # Frequency band configuration, defines number of frequency bins per band
    - 2                 # First 24 bands each contain 2 frequency bins
    - 2
    - 2
    - 2
    - 2
    - 2
    - 2
    - 2
    - 2
    - 2
    - 2
    - 2
    - 2
    - 2
    - 2
    - 2
    - 2
    - 2
    - 2
    - 2
    - 2
    - 2
    - 2
    - 2
    - 4                 # Next 12 bands each contain 4 frequency bins
    - 4
    - 4
    - 4
    - 4
    - 4
    - 4
    - 4
    - 4
    - 4
    - 4
    - 4
    - 12               # Next 8 bands each contain 12 frequency bins
    - 12
    - 12
    - 12
    - 12
    - 12
    - 12
    - 12
    - 24               # Next 8 bands each contain 24 frequency bins
    - 24
    - 24
    - 24
    - 24
    - 24
    - 24
    - 24
    - 48               # Next 8 bands each contain 48 frequency bins
    - 48
    - 48
    - 48
    - 48
    - 48
    - 48
    - 48
    - 128              # Last two bands contain 128 and 129 frequency bins
    - 129
  stereo: false         # Whether to use stereo, false for mono
  num_stems: 1          # Number of sources to separate, 1 for vocals only
  time_transformer_depth: 1    # Number of time-dimension Transformer layers
  freq_transformer_depth: 1    # Number of frequency-dimension Transformer layers
  linear_transformer_depth: 0  # Number of linear attention layers, 0 to disable
  attn_dropout: 0.1     # Attention layer dropout rate
  ff_dropout: 0.1       # Feedforward network dropout rate
  dim_freqs_in: 1025    # Input frequency dimension
  stft_n_fft: 2048      # STFT FFT size
  stft_hop_length: 512  # STFT hop length
  stft_win_length: 2048 # STFT window length
  stft_normalized: false # Whether to normalize STFT output
  mask_estimator_depth: 2  # Mask estimator depth
  multi_stft_resolution_loss_weight: 1.0  # Multi-resolution STFT loss weight
  multi_stft_resolutions_window_sizes: !!python/tuple  # Multi-resolution STFT window size configuration
    - 4096
    - 2048
    - 1024
    - 512
    - 256
  multi_stft_hop_size: 147    # Multi-resolution STFT hop length
  multi_stft_normalized: False # Whether to normalize multi-resolution STFT output

training:
  batch_size: 12  # Number of samples per batch
  gradient_accumulation_steps: 1  # Gradient accumulation steps
  grad_clip: 0  # Gradient clipping threshold
  instruments:  # Instruments used in training
    - vocals  # Vocals
    - noise  # Noise
  lr: 5.0e-4  # Learning rate
  patience: 2  # Learning rate scheduler patience
  reduce_factor: 0.95  # Learning rate reduction factor
  target_instrument: vocals  # Target instrument
  num_epochs: 1000  # Number of training epochs
  num_steps: 200  # Steps per epoch
  q: 0.95  # Quantile parameter
  coarse_loss_clip: true  # Whether to clip coarse loss
  ema_momentum: 0.999  # Exponential moving average momentum
  optimizer: adamw  # Optimizer type
  other_fix: false  # Other fix flag
  use_amp: false  # Whether to use automatic mixed precision
  early_stop:  # Early stopping configuration
    enabled: true  # Whether to enable early stopping
    patience: 30  # Early stopping patience
    metric: "si-sdr"  # Metric for early stopping

inference:
  batch_size: 10           # Number of samples per batch
  num_overlap: 4           # Number of overlaps
